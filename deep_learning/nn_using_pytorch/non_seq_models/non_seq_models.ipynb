{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f50c2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchmetrics\n",
    "from torch.utils.data import TensorDataset , DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c12f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19c4a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other libraries \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4c8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c2836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets scale\n",
    "\n",
    "scale_data = StandardScaler()\n",
    "housing_data = scale_data.fit_transform(housing.data)\n",
    "\n",
    "scale_target = StandardScaler()\n",
    "housing_target= scale_target.fit_transform(housing.target.reshape(-1 , 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb11900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data splittinh\n",
    "\n",
    "x_train_full , x_test , y_train_full , y_test = train_test_split(housing_data , housing_target ,\n",
    "                                                                  random_state=42 , test_size=0.15)\n",
    "\n",
    "x_train , x_valid , y_train , y_valid = train_test_split(x_train_full , y_train_full ,\n",
    "                                                         random_state=42 , test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24010329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to tensors \n",
    "\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "x_valid = torch.FloatTensor(x_valid)\n",
    "\n",
    "y_train = torch.FloatTensor(y_train).reshape(-1  ,1 )\n",
    "y_test = torch.FloatTensor(y_test).reshape(-1  ,1 )\n",
    "y_valid = torch.FloatTensor(y_valid).reshape(-1  ,1 )\n",
    "\n",
    "\n",
    "\n",
    "n_features = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b11320ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeep(nn.Module):\n",
    "    def __init__(self , n_features):\n",
    "        super().__init__()\n",
    "        self.deep_stack = nn.Sequential(\n",
    "            nn.Linear(n_features , 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50 , 40),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(40 + n_features , 1)\n",
    "        \n",
    "    def forward(self , x):\n",
    "        deep_output = self.deep_stack(x)\n",
    "        wide_and_deep = torch.concat([x , deep_output] , dim=1)\n",
    "        return self.output_layer(wide_and_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a95881ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "712a332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model , optimizer ,criterion ,train_loader , n_epoch):\n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss = 0\n",
    "        for x_batch , y_batch in train_loader:\n",
    "            model.train()\n",
    "            #on gpu\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            #forward\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            #loss\n",
    "            loss =  criterion(y_pred , y_batch)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            #backward\n",
    "            loss.backward()\n",
    "            \n",
    "            #optimizer \n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        mean_loss = total_loss/len(train_loader)\n",
    "        \n",
    "        print(f\"Epoch : {epoch + 1}/{n_epoch}\",\n",
    "              f\"Loss : {mean_loss:.4f}\")\n",
    "        \n",
    "        \n",
    "def evaluate(model , metrics , valid_loader):\n",
    "    model.eval()\n",
    "    metrics.reset()\n",
    "    with torch.no_grad():\n",
    "        for x_batch , y_batch in valid_loader:\n",
    "            x_batch , y_batch = x_batch.to(device) , y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            metrics.update(y_pred , y_batch)\n",
    "    \n",
    "    return metrics.compute()\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8004a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/20 Loss : 0.8552\n",
      "Epoch : 2/20 Loss : 0.5839\n",
      "Epoch : 3/20 Loss : 0.5115\n",
      "Epoch : 4/20 Loss : 0.4899\n",
      "Epoch : 5/20 Loss : 0.4714\n",
      "Epoch : 6/20 Loss : 0.4616\n",
      "Epoch : 7/20 Loss : 0.4533\n",
      "Epoch : 8/20 Loss : 0.4458\n",
      "Epoch : 9/20 Loss : 0.4404\n",
      "Epoch : 10/20 Loss : 0.4340\n",
      "Epoch : 11/20 Loss : 0.4285\n",
      "Epoch : 12/20 Loss : 0.4252\n",
      "Epoch : 13/20 Loss : 0.4213\n",
      "Epoch : 14/20 Loss : 0.4153\n",
      "Epoch : 15/20 Loss : 0.4136\n",
      "Epoch : 16/20 Loss : 0.4094\n",
      "Epoch : 17/20 Loss : 0.4058\n",
      "Epoch : 18/20 Loss : 0.4025\n",
      "Epoch : 19/20 Loss : 0.4024\n",
      "Epoch : 20/20 Loss : 0.4003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6277, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = WideAndDeep(n_features).to(device)\n",
    "learning_rate = 0.002\n",
    "n_epoch = 20\n",
    "\n",
    "train_dataset = TensorDataset(x_train , y_train)\n",
    "train_loader = DataLoader(train_dataset , batch_size=100 , pin_memory=True , shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(x_valid , y_valid)\n",
    "valid_loader = DataLoader(valid_dataset , batch_size=100 , pin_memory=True , shuffle=True)\n",
    "\n",
    "mse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model.parameters() , lr=learning_rate)\n",
    "\n",
    "train(model , optimizer , loss , train_loader , n_epoch )\n",
    "evaluate(model , mse , valid_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b201f043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/20 Loss : 0.8552 Train Metrics : 0.9266 valid metrics : 0.8128\n",
      "Epoch : 2/20 Loss : 0.5837 Train Metrics : 0.7652 valid metrics : 0.7333\n",
      "Epoch : 3/20 Loss : 0.5117 Train Metrics : 0.7161 valid metrics : 0.7053\n",
      "Epoch : 4/20 Loss : 0.4852 Train Metrics : 0.6973 valid metrics : 0.6924\n",
      "Epoch : 5/20 Loss : 0.4721 Train Metrics : 0.6873 valid metrics : 0.6836\n",
      "Epoch : 6/20 Loss : 0.4614 Train Metrics : 0.6801 valid metrics : 0.6771\n",
      "Epoch : 7/20 Loss : 0.4553 Train Metrics : 0.6740 valid metrics : 0.6710\n",
      "Epoch : 8/20 Loss : 0.4477 Train Metrics : 0.6687 valid metrics : 0.6658\n",
      "Epoch : 9/20 Loss : 0.4398 Train Metrics : 0.6638 valid metrics : 0.6608\n",
      "Epoch : 10/20 Loss : 0.4355 Train Metrics : 0.6593 valid metrics : 0.6564\n",
      "Epoch : 11/20 Loss : 0.4286 Train Metrics : 0.6554 valid metrics : 0.6524\n",
      "Epoch : 12/20 Loss : 0.4239 Train Metrics : 0.6518 valid metrics : 0.6486\n",
      "Epoch : 13/20 Loss : 0.4191 Train Metrics : 0.6485 valid metrics : 0.6453\n",
      "Epoch : 14/20 Loss : 0.4162 Train Metrics : 0.6454 valid metrics : 0.6420\n",
      "Epoch : 15/20 Loss : 0.4122 Train Metrics : 0.6428 valid metrics : 0.6392\n",
      "Epoch : 16/20 Loss : 0.4099 Train Metrics : 0.6401 valid metrics : 0.6366\n",
      "Epoch : 17/20 Loss : 0.4063 Train Metrics : 0.6376 valid metrics : 0.6342\n",
      "Epoch : 18/20 Loss : 0.4050 Train Metrics : 0.6357 valid metrics : 0.6319\n",
      "Epoch : 19/20 Loss : 0.4005 Train Metrics : 0.6336 valid metrics : 0.6298\n",
      "Epoch : 20/20 Loss : 0.3988 Train Metrics : 0.6320 valid metrics : 0.6277\n"
     ]
    }
   ],
   "source": [
    "def train_and_eval(model , optimizer , criterion  , metric , train_loader , \n",
    "                   valid_loader , n_epoch ):\n",
    "    \n",
    "    history = {\"train_metrics\" : [] , \"val_metrics\" : [] , \"loss\" : []}\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        metric.reset()\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for x_batch , y_batch in train_loader:\n",
    "            model.train()\n",
    "            x_batch , y_batch = x_batch.to(device) , y_batch.to(device)\n",
    "            \n",
    "            #forward \n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            #loss  \n",
    "            loss = criterion(y_pred , y_batch)\n",
    "            total_loss+= loss.item()\n",
    "            \n",
    "            #optimzer and backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            metric.update(y_pred , y_batch)\n",
    "        \n",
    "        mean_loss = total_loss/len(train_loader)\n",
    "        history[\"loss\"].append(mean_loss)\n",
    "        history[\"train_metrics\"].append(metric.compute().item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            metric.reset()\n",
    "            \n",
    "            for x_valid_batch , y_valid_batch in valid_loader:\n",
    "                x_valid_batch , y_valid_batch = x_valid_batch.to(device) , y_valid_batch.to(device)\n",
    "                y_pred = model(x_valid_batch)\n",
    "                metric.update(y_pred , y_valid_batch)\n",
    "            \n",
    "        history[\"val_metrics\"].append(metric.compute().item())\n",
    "        \n",
    "        print(f\"Epoch : {epoch+1}/{n_epoch}\",\n",
    "              f\"Loss : {history['loss'][-1]:.4f}\",\n",
    "              f\"Train Metrics : {history['train_metrics'][-1]:.4f}\",\n",
    "              f\"valid metrics : {history['val_metrics'][-1]:.4f}\")\n",
    "        \n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = WideAndDeep(n_features).to(device)\n",
    "learning_rate = 0.002\n",
    "n_epoch = 20\n",
    "\n",
    "train_dataset = TensorDataset(x_train , y_train)\n",
    "train_loader = DataLoader(train_dataset , batch_size=100 , pin_memory=True , shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(x_valid , y_valid)\n",
    "valid_loader = DataLoader(valid_dataset , batch_size=100 , pin_memory=True , shuffle=True)\n",
    "\n",
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model.parameters() , lr=learning_rate)\n",
    "\n",
    "history = train_and_eval(model , optimizer , loss , rmse , train_loader , \n",
    "               valid_loader , n_epoch)\n",
    "\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72e63998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/20 Loss : 0.9370 Train Metrics : 0.9677 valid metrics : 0.8529\n",
      "Epoch : 2/20 Loss : 0.6365 Train Metrics : 0.7955 valid metrics : 0.7565\n",
      "Epoch : 3/20 Loss : 0.5433 Train Metrics : 0.7356 valid metrics : 0.7223\n",
      "Epoch : 4/20 Loss : 0.5099 Train Metrics : 0.7152 valid metrics : 0.7097\n",
      "Epoch : 5/20 Loss : 0.5062 Train Metrics : 0.7072 valid metrics : 0.7033\n",
      "Epoch : 6/20 Loss : 0.4990 Train Metrics : 0.7029 valid metrics : 0.6996\n",
      "Epoch : 7/20 Loss : 0.4903 Train Metrics : 0.6998 valid metrics : 0.6965\n",
      "Epoch : 8/20 Loss : 0.4853 Train Metrics : 0.6974 valid metrics : 0.6943\n",
      "Epoch : 9/20 Loss : 0.4816 Train Metrics : 0.6952 valid metrics : 0.6922\n",
      "Epoch : 10/20 Loss : 0.4791 Train Metrics : 0.6931 valid metrics : 0.6898\n",
      "Epoch : 11/20 Loss : 0.4784 Train Metrics : 0.6912 valid metrics : 0.6877\n",
      "Epoch : 12/20 Loss : 0.4751 Train Metrics : 0.6896 valid metrics : 0.6857\n",
      "Epoch : 13/20 Loss : 0.4748 Train Metrics : 0.6877 valid metrics : 0.6840\n",
      "Epoch : 14/20 Loss : 0.4690 Train Metrics : 0.6860 valid metrics : 0.6824\n",
      "Epoch : 15/20 Loss : 0.4731 Train Metrics : 0.6845 valid metrics : 0.6804\n",
      "Epoch : 16/20 Loss : 0.4647 Train Metrics : 0.6830 valid metrics : 0.6790\n",
      "Epoch : 17/20 Loss : 0.4633 Train Metrics : 0.6815 valid metrics : 0.6776\n",
      "Epoch : 18/20 Loss : 0.4614 Train Metrics : 0.6800 valid metrics : 0.6762\n",
      "Epoch : 19/20 Loss : 0.4613 Train Metrics : 0.6787 valid metrics : 0.6747\n",
      "Epoch : 20/20 Loss : 0.4582 Train Metrics : 0.6773 valid metrics : 0.6734\n"
     ]
    }
   ],
   "source": [
    "class WideAndDeepV2(nn.Module):\n",
    "    def __init__(self , n_features):\n",
    "        super().__init__()\n",
    "        self.deep_stack = nn.Sequential(\n",
    "            nn.Linear(n_features - 2 , 50) , nn.ReLU(),\n",
    "            nn.Linear(50 , 40) , nn.ReLU() , \n",
    "            nn.Linear(40 , 30)  , nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.output_layer=nn.Linear(30 + 5 , 1)\n",
    "        \n",
    "    def forward(self , x):\n",
    "        x_wide = x[: , :5]\n",
    "        x_deep = x[: , 2:]\n",
    "        deep_output = self.deep_stack(x_deep)\n",
    "        wide_and_deep = torch.concat([x_wide , deep_output] , dim=1)\n",
    "        return self.output_layer(wide_and_deep)\n",
    "    \n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = WideAndDeepV2(n_features).to(device)\n",
    "learning_rate = 0.002\n",
    "n_epoch = 20\n",
    "\n",
    "train_dataset = TensorDataset(x_train , y_train)\n",
    "train_loader = DataLoader(train_dataset , batch_size=100 , pin_memory=True , shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(x_valid , y_valid)\n",
    "valid_loader = DataLoader(valid_dataset , batch_size=100 , pin_memory=True , shuffle=True)\n",
    "\n",
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model.parameters() , lr=learning_rate)\n",
    "\n",
    "history = train_and_eval(model , optimizer , loss , rmse , train_loader , \n",
    "               valid_loader , n_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "819d176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when input size is different \n",
    "#or wide and deep with multiple inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24148082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepV3(nn.Module):\n",
    "    def __init__(self , n_features):\n",
    "        super().__init__()\n",
    "        self.deep_stack = nn.Sequential(\n",
    "            nn.Linear(n_features - 2 , 50) , nn.ReLU() ,\n",
    "            nn.Linear(50 , 40) , nn.ReLU(),\n",
    "            nn.Linear(40 , 30) , nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(30 + 5 , 1)\n",
    "        \n",
    "    def forward(self , x_wide , x_deep):\n",
    "        deep_output = self.deep_stack(x_deep)\n",
    "        wide_and_deep = torch.concat([x_wide , deep_output] , dim=1)\n",
    "        return self.output_layer(wide_and_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a468759",
   "metadata": {},
   "outputs": [],
   "source": [
    "trian_data_wd = TensorDataset(x_train[: , :5] , x_train[: , 2:] , y_train)\n",
    "train_loader_wd = DataLoader(trian_data_wd , shuffle=True , batch_size= 200 , pin_memory=True)\n",
    "\n",
    "val_data_wd = TensorDataset(x_valid[: , :5] , x_valid[: , 2:] , y_valid)\n",
    "val_loader_wd = DataLoader(val_data_wd , shuffle=True , batch_size= 200 , pin_memory=True)\n",
    "\n",
    "test_data_wd = TensorDataset(x_test[: , :5] , x_test[: , 2:] , y_test)\n",
    "test_loader_wd = DataLoader(test_data_wd , shuffle=True , pin_memory=True , batch_size= 200)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcc8a439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/20 Loss : 1.0636 Train Metrics : 1.0315 valid metrics : 0.9539\n",
      "Epoch : 2/20 Loss : 0.8074 Train Metrics : 0.8985 valid metrics : 0.8530\n",
      "Epoch : 3/20 Loss : 0.6698 Train Metrics : 0.8180 valid metrics : 0.7920\n",
      "Epoch : 4/20 Loss : 0.5934 Train Metrics : 0.7706 valid metrics : 0.7561\n",
      "Epoch : 5/20 Loss : 0.5541 Train Metrics : 0.7432 valid metrics : 0.7350\n",
      "Epoch : 6/20 Loss : 0.5296 Train Metrics : 0.7273 valid metrics : 0.7224\n",
      "Epoch : 7/20 Loss : 0.5163 Train Metrics : 0.7180 valid metrics : 0.7146\n",
      "Epoch : 8/20 Loss : 0.5065 Train Metrics : 0.7122 valid metrics : 0.7096\n",
      "Epoch : 9/20 Loss : 0.5024 Train Metrics : 0.7084 valid metrics : 0.7062\n",
      "Epoch : 10/20 Loss : 0.4987 Train Metrics : 0.7057 valid metrics : 0.7035\n",
      "Epoch : 11/20 Loss : 0.4948 Train Metrics : 0.7036 valid metrics : 0.7014\n",
      "Epoch : 12/20 Loss : 0.4926 Train Metrics : 0.7020 valid metrics : 0.6996\n",
      "Epoch : 13/20 Loss : 0.4912 Train Metrics : 0.7005 valid metrics : 0.6981\n",
      "Epoch : 14/20 Loss : 0.4879 Train Metrics : 0.6992 valid metrics : 0.6968\n",
      "Epoch : 15/20 Loss : 0.4885 Train Metrics : 0.6980 valid metrics : 0.6954\n",
      "Epoch : 16/20 Loss : 0.4855 Train Metrics : 0.6968 valid metrics : 0.6941\n",
      "Epoch : 17/20 Loss : 0.4848 Train Metrics : 0.6957 valid metrics : 0.6930\n",
      "Epoch : 18/20 Loss : 0.4820 Train Metrics : 0.6946 valid metrics : 0.6919\n",
      "Epoch : 19/20 Loss : 0.4811 Train Metrics : 0.6936 valid metrics : 0.6909\n",
      "Epoch : 20/20 Loss : 0.4807 Train Metrics : 0.6926 valid metrics : 0.6898\n"
     ]
    }
   ],
   "source": [
    "def train_and_eval_modified(model , optimizer , criterion  , metric , train_loader , \n",
    "                   valid_loader , n_epoch ):\n",
    "    \n",
    "    history = {\"train_metrics\" : [] , \"val_metrics\" : [] , \"loss\" : []}\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        metric.reset()\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for x_batch_wide , x_batch_deep , y_batch in train_loader:\n",
    "            model.train()\n",
    "            x_batch_wide , x_batch_deep , y_batch = x_batch_wide.to(device) , x_batch_deep.to(device) ,  y_batch.to(device)\n",
    "            \n",
    "            #forward \n",
    "            y_pred = model(x_batch_wide , x_batch_deep)\n",
    "            \n",
    "            #loss  \n",
    "            loss = criterion(y_pred , y_batch)\n",
    "            total_loss+= loss.item()\n",
    "            \n",
    "            #optimzer and backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            metric.update(y_pred , y_batch)\n",
    "        \n",
    "        mean_loss = total_loss/len(train_loader)\n",
    "        history[\"loss\"].append(mean_loss)\n",
    "        history[\"train_metrics\"].append(metric.compute().item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            metric.reset()\n",
    "            \n",
    "            for x_valid_batch_wide , x_valid_batch_deep , y_valid_batch in valid_loader:\n",
    "                x_valid_batch_wide , x_valid_batch_deep , y_valid_batch = x_valid_batch_wide.to(device) , x_valid_batch_deep.to(device) , y_valid_batch.to(device)\n",
    "                y_pred = model(x_valid_batch_wide , x_valid_batch_deep)\n",
    "                metric.update(y_pred , y_valid_batch)\n",
    "            \n",
    "        history[\"val_metrics\"].append(metric.compute().item())\n",
    "        \n",
    "        print(f\"Epoch : {epoch+1}/{n_epoch}\",\n",
    "              f\"Loss : {history['loss'][-1]:.4f}\",\n",
    "              f\"Train Metrics : {history['train_metrics'][-1]:.4f}\",\n",
    "              f\"valid metrics : {history['val_metrics'][-1]:.4f}\")\n",
    "        \n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = WideAndDeepV3(n_features).to(device)\n",
    "learning_rate = 0.002\n",
    "n_epoch = 20\n",
    "\n",
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model.parameters() , lr=learning_rate)\n",
    "\n",
    "history = train_and_eval_modified(model , optimizer , loss , rmse , train_loader_wd , \n",
    "               val_loader_wd , n_epoch)\n",
    "\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77bc58a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naming each input \n",
    "\n",
    "class WideAndDeepDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self , x_wide , x_deep , y):\n",
    "        self.x_wide = x_wide\n",
    "        self.x_deep = x_deep\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self , idx):\n",
    "        input_dict = {\"x_wide\" : self.x_wide[idx] , \"x_deep\" : self.x_deep[idx]}\n",
    "        return input_dict , self.y[idx]\n",
    "    \n",
    "    \n",
    "train_data_named = WideAndDeepDataset(\n",
    "    x_wide = x_train[: , :5] , x_deep = x_train[: , 2:] , y=y_train\n",
    ")\n",
    "\n",
    "train_loader_named = DataLoader(train_data_named , batch_size = 200 , shuffle=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41d79d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllibraries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
